{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube åŠå¤šå¹³å°ä¸‹è¼‰æ–¹æ¡ˆ - å¯¦éš›æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦ä¸‰ç¨®ä¸‹è¼‰æ–¹æ¡ˆåœ¨ä¸åŒå¹³å°çš„å¯¦éš›è¡¨ç¾ï¼š\n",
    "\n",
    "1. **pytubefix** - YouTube å°ˆç”¨\n",
    "2. **yt-dlp** - å¤šå¹³å°é€šç”¨\n",
    "3. **Playwright** - ç€è¦½å™¨æ¨¡æ“¬ï¼ˆå‚™é¸ï¼‰\n",
    "\n",
    "## æ¸¬è©¦ URL åˆ—è¡¨\n",
    "\n",
    "- YouTube MP3: https://www.youtube.com/watch?v=0rp3pP2Xwhs\n",
    "- YouTube MP4: https://www.youtube.com/shorts/vGmdigoUJ6c\n",
    "- Facebook Video: https://www.facebook.com/share/r/1Bx14m8c7b/\n",
    "- Instagram Video: https://www.instagram.com/reel/DRtrroMkeDU/\n",
    "- X (Twitter) Video: https://x.com/Appraiser008/status/1995090648689717458\n",
    "- **Threads å…¬é–‹å½±ç‰‡**: https://www.threads.com/@zuck/post/DHV7vTivqWD\n",
    "- **Threads è¼ªæ’­è²¼æ–‡**: https://www.threads.com/@zuck/post/DJDhoQfxb43\n",
    "\n",
    "## Threads ä¸‹è¼‰ç ”ç©¶æ‘˜è¦\n",
    "\n",
    "æ ¹æ“š yt-dlp GitHub issues ç ”ç©¶çµæœï¼š\n",
    "\n",
    "### ç›®å‰ç‹€æ…‹\n",
    "- **yt-dlp å®˜æ–¹å°šæœªåˆä½µ Threads æ”¯æ´** (æˆªè‡³ 2024/12)\n",
    "- æœ‰å…©å€‹é–‹æ”¾çš„ PR æ­£åœ¨é€²è¡Œä¸­:\n",
    "  - [PR #9852](https://github.com/yt-dlp/yt-dlp/pull/9852) - åŸºç¤ Threads extractor\n",
    "  - [PR #13512](https://github.com/yt-dlp/yt-dlp/pull/13512) - æ›´å®Œæ•´çš„ç‰ˆæœ¬ï¼Œæ”¯æ´ç™»å…¥\n",
    "\n",
    "### ç™»å…¥/èªè­‰å•é¡Œ\n",
    "Threads å°éƒ¨åˆ†å…§å®¹éœ€è¦ç™»å…¥é©—è­‰ï¼š\n",
    "1. **å…¬é–‹è²¼æ–‡** - å¯ç›´æ¥ä¸‹è¼‰ï¼Œç„¡éœ€ç™»å…¥\n",
    "2. **ç§äººè²¼æ–‡** - éœ€è¦é€é cookies èªè­‰\n",
    "3. **èªè­‰æ–¹å¼**: ä½¿ç”¨ `--cookies` æˆ– `--cookies-from-browser` åƒæ•¸\n",
    "\n",
    "### æš«æ™‚è§£æ±ºæ–¹æ¡ˆ\n",
    "1. **ä½¿ç”¨é–‹ç™¼ç‰ˆ yt-dlp** - å¾ PR branch å®‰è£\n",
    "2. **æ‰‹å‹•ä½¿ç”¨ Threads PR çš„ extractor** - è¤‡è£½ threads.py åˆ°æœ¬åœ°\n",
    "3. **ä½¿ç”¨ Instagram cookies** - Threads èˆ‡ Instagram å…±ç”¨èªè­‰ (åŒå±¬ Meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# è¨­ç½®è¼¸å‡ºç›®éŒ„\n",
    "OUTPUT_DIR = Path('./downloads')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# å®šç¾©æ¸¬è©¦ URL\n",
    "TEST_URLS = {\n",
    "    \"youtube_mp3\": \"https://www.youtube.com/watch?v=0rp3pP2Xwhs\",\n",
    "    \"youtube_mp4\": \"https://www.youtube.com/shorts/vGmdigoUJ6c\",\n",
    "    \"facebook\": \"https://www.facebook.com/share/r/1Bx14m8c7b/\",\n",
    "    \"instagram\": \"https://www.instagram.com/reel/DRtrroMkeDU/?utm_source=ig_web_copy_link&igsh=NTc4MTIwNjQ2YQ==\",\n",
    "    \"twitter\": \"https://x.com/Appraiser008/status/1995090648689717458?s=20\",\n",
    "    # Threads æ¸¬è©¦ URL\n",
    "    \"threads_public\": \"https://www.threads.com/@zuck/post/DHV7vTivqWD\",  # å…¬é–‹å½±ç‰‡è²¼æ–‡\n",
    "    \"threads_private\": \"https://www.threads.com/@thuy.11.2/post/DRysaLcgNsN?xmt=AQF08iq6Xg6BuyiMMn-9T5oiGmQ6346nnycIlzAzr5C5SQ\",  # å¤šåœ–ç‰‡/å½±ç‰‡è¼ªæ’­\n",
    "}\n",
    "\n",
    "print(f\"è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"\\næ¸¬è©¦ URL:\")\n",
    "for platform, url in TEST_URLS.items():\n",
    "    print(f\"  {platform:20}: {url}\")\n",
    "\n",
    "# æ¸¬è©¦çµæœè¨˜éŒ„\n",
    "test_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytubefix æ¸¬è©¦\n",
    "\n",
    "pytubefix æ˜¯ pytube çš„ç¶­è­·ç‰ˆæœ¬ï¼Œæ¸¬è©¦ YouTube ä¸‹è¼‰æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ pytubefix\n",
    "!pip install -q pytubefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def test_pytubefix_youtube_mp3():\n",
    "    \"\"\"æ¸¬è©¦ pytubefix YouTube MP3 ä¸‹è¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ§ª æ¸¬è©¦ 1: pytubefix - YouTube MP3 ä¸‹è¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    url = TEST_URLS['youtube_mp3']\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"URL: {url}\\n\")\n",
    "        \n",
    "        # å»ºç«‹ YouTube ç‰©ä»¶\n",
    "        yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        \n",
    "        print(f\"æ¨™é¡Œ: {yt.title}\")\n",
    "        print(f\"é•·åº¦: {yt.length} ç§’\")\n",
    "        print(f\"ä½œè€…: {yt.author}\")\n",
    "        \n",
    "        # æ¸…ç†æª”å\n",
    "        safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', yt.title)\n",
    "        \n",
    "        # ä¸‹è¼‰éŸ³è¨Š\n",
    "        audio_stream = yt.streams.get_audio_only()\n",
    "        print(f\"éŸ³è¨Šä¸²æµ: {audio_stream}\")\n",
    "        \n",
    "        output_file = audio_stream.download(\n",
    "            output_path=str(OUTPUT_DIR),\n",
    "            filename=f\"{safe_title}.m4a\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ä¸‹è¼‰å®Œæˆ (m4a): {output_file}\")\n",
    "        \n",
    "        # è½‰æ›ç‚º MP3\n",
    "        try:\n",
    "            mp3_file = str(OUTPUT_DIR / f\"{safe_title}.mp3\")\n",
    "            subprocess.run(\n",
    "                ['ffmpeg', '-i', output_file, '-q:a', '0', '-map', 'a', mp3_file],\n",
    "                capture_output=True,\n",
    "                check=True\n",
    "            )\n",
    "            Path(output_file).unlink()\n",
    "            print(f\"âœ… è½‰æ›æˆåŠŸ (MP3): {mp3_file}\")\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            file_size = Path(mp3_file).stat().st_size / (1024 * 1024)\n",
    "            \n",
    "            test_results['pytubefix_youtube_mp3'] = {\n",
    "                'status': 'âœ… æˆåŠŸ',\n",
    "                'time': f\"{elapsed_time:.1f}s\",\n",
    "                'size': f\"{file_size:.2f}MB\",\n",
    "                'file': mp3_file\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except (FileNotFoundError, subprocess.CalledProcessError) as e:\n",
    "            print(f\"âš ï¸ ffmpeg ä¸å¯ç”¨: {e}\")\n",
    "            test_results['pytubefix_youtube_mp3'] = {\n",
    "                'status': 'âš ï¸ éƒ¨åˆ†æˆåŠŸ (m4a)',\n",
    "                'file': output_file\n",
    "            }\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        test_results['pytubefix_youtube_mp3'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {str(e)[:50]}'\n",
    "        }\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_pytubefix_youtube_mp4():\n",
    "    \"\"\"æ¸¬è©¦ pytubefix YouTube MP4 ä¸‹è¼‰ï¼ˆæœ€é«˜å“è³ªï¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ§ª æ¸¬è©¦: pytubefix - YouTube MP4 ä¸‹è¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    url = TEST_URLS['youtube_mp4']\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"URL: {url}\\n\")\n",
    "        \n",
    "        # å»ºç«‹ YouTube ç‰©ä»¶\n",
    "        yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        \n",
    "        print(f\"æ¨™é¡Œ: {yt.title}\")\n",
    "        print(f\"é•·åº¦: {yt.length} ç§’\")\n",
    "        \n",
    "        # æ¸…ç†æª”å\n",
    "        safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', yt.title)\n",
    "        \n",
    "        # å–å¾—æœ€é«˜å“è³ªçš„è¦–è¨Šï¼ˆåŒ…å«éŸ³è¨Šï¼‰\n",
    "        # å…ˆæ‰¾æœ€é«˜å“è³ªçš„ progressiveï¼ˆè‡ªå¸¶éŸ³è¨Šï¼‰æµ\n",
    "        stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "        \n",
    "        if stream is None:\n",
    "            # å¦‚æœæ²’æœ‰ progressiveï¼Œå‰‡åˆ†åˆ¥ä¸‹è¼‰è¦–è¨Šå’ŒéŸ³è¨Šå¾Œåˆä½µ\n",
    "            print(\"âš ï¸ æ²’æœ‰ progressive æµï¼Œå˜—è©¦åˆ†åˆ¥ä¸‹è¼‰è¦–è¨Šå’ŒéŸ³è¨Š...\")\n",
    "            \n",
    "            # ä¸‹è¼‰æœ€é«˜å“è³ªè¦–è¨Š\n",
    "            video_stream = yt.streams.filter(mime_type='video/mp4').order_by('resolution').desc().first()\n",
    "            audio_stream = yt.streams.filter(mime_type='audio/mp4').order_by('abr').desc().first()\n",
    "            \n",
    "            if video_stream is None or audio_stream is None:\n",
    "                print(\"âŒ æ‰¾ä¸åˆ°é©åˆçš„è¦–è¨Šæˆ–éŸ³è¨Šæµ\")\n",
    "                test_results['pytubefix_youtube_mp4'] = {'status': 'âŒ å¤±æ•—: æ‰¾ä¸åˆ°è¦–è¨Šæˆ–éŸ³è¨Šæµ'}\n",
    "                return False\n",
    "            \n",
    "            print(f\"è¦–è¨Š: {video_stream}\")\n",
    "            print(f\"éŸ³è¨Š: {audio_stream}\")\n",
    "            \n",
    "            # ä¸‹è¼‰è¦–è¨Š\n",
    "            video_file = video_stream.download(\n",
    "                output_path=str(OUTPUT_DIR),\n",
    "                filename=f\"{safe_title}_video.mp4\"\n",
    "            )\n",
    "            \n",
    "            # ä¸‹è¼‰éŸ³è¨Š\n",
    "            audio_file = audio_stream.download(\n",
    "                output_path=str(OUTPUT_DIR),\n",
    "                filename=f\"{safe_title}_audio.m4a\"\n",
    "            )\n",
    "            \n",
    "            # åˆä½µè¦–è¨Šå’ŒéŸ³è¨Š\n",
    "            final_file = str(OUTPUT_DIR / f\"{safe_title}.mp4\")\n",
    "            cmd = [\n",
    "                'ffmpeg',\n",
    "                '-i', video_file,\n",
    "                '-i', audio_file,\n",
    "                '-c:v', 'copy',\n",
    "                '-c:a', 'aac',\n",
    "                '-strict', 'experimental',\n",
    "                final_file\n",
    "            ]\n",
    "            \n",
    "            subprocess.run(cmd, capture_output=True, check=True)\n",
    "            \n",
    "            # åˆªé™¤è‡¨æ™‚æ–‡ä»¶\n",
    "            Path(video_file).unlink()\n",
    "            Path(audio_file).unlink()\n",
    "            \n",
    "            print(f\"âœ… åˆä½µæˆåŠŸ: {final_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âœ… æ‰¾åˆ°æœ€é«˜å“è³ªæµ: {stream}\")\n",
    "            \n",
    "            final_file = stream.download(\n",
    "                output_path=str(OUTPUT_DIR),\n",
    "                filename=f\"{safe_title}.mp4\"\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… ä¸‹è¼‰å®Œæˆ: {final_file}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        file_size = Path(final_file).stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        test_results['pytubefix_youtube_mp4'] = {\n",
    "            'status': 'âœ… æˆåŠŸ',\n",
    "            'time': f\"{elapsed_time:.1f}s\",\n",
    "            'size': f\"{file_size:.2f}MB\",\n",
    "            'file': final_file\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        test_results['pytubefix_youtube_mp4'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {str(e)[:50]}'\n",
    "        }\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_pytubefix_youtube_mp3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yt-dlp å¤šå¹³å°æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦ yt-dlp åœ¨å„å¹³å°çš„ä¸‹è¼‰èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_mp4_for_mobile(input_file, output_file=None, target_bitrate='1000k', target_height=720):\n",
    "    \"\"\"\n",
    "    è½‰ç¢¼ MP4 ä»¥é©é…æ‰‹æ©Ÿåˆ†äº«\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        input_file: è¼¸å…¥ MP4 æ–‡ä»¶\n",
    "        output_file: è¼¸å‡ºæ–‡ä»¶ (default: input_file + _mobile.mp4)\n",
    "        target_bitrate: ç›®æ¨™æ¯”ç‰¹ç‡ (default: 1000k)\n",
    "        target_height: ç›®æ¨™é«˜åº¦ (default: 720p)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    if output_file is None:\n",
    "        output_file = str(input_path.parent / f\"{input_path.stem}_mobile.mp4\")\n",
    "    \n",
    "    print(\"\\nğŸ“¹ è½‰ç¢¼ MP4 ä»¥é©é…æ‰‹æ©Ÿåˆ†äº«...\")\n",
    "    print(f\"   è¼¸å…¥: {input_path.name}\")\n",
    "    print(f\"   è¼¸å‡º: {Path(output_file).name}\")\n",
    "    print(f\"   ç›®æ¨™: {target_height}p @ {target_bitrate}\")\n",
    "    \n",
    "    try:\n",
    "        cmd = [\n",
    "            'ffmpeg',\n",
    "            '-i', str(input_file),\n",
    "            '-vf', f'scale=-2:{target_height}',  # ä¿æŒç¸±æ©«æ¯”ï¼Œé«˜åº¦ç‚º target_heightï¼Œå¯¬åº¦å¿…é ˆèƒ½è¢« 2 æ•´é™¤\n",
    "            '-b:v', target_bitrate,\n",
    "            '-b:a', '128k',\n",
    "            '-c:v', 'libx264',\n",
    "            '-c:a', 'aac',\n",
    "            '-crf', '23',  # è³ªé‡ (0-51, é è¨­ 23, è¶Šä½è¶Šå¥½)\n",
    "            '-preset', 'medium',  # ç·¨ç¢¼é€Ÿåº¦\n",
    "            output_file\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            output_size = Path(output_file).stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ… è½‰ç¢¼æˆåŠŸ: {output_size:.2f}MB\")\n",
    "            return output_file\n",
    "        else:\n",
    "            print(f\"âŒ è½‰ç¢¼å¤±æ•—: {result.stderr[:200]}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"âŒ è½‰ç¢¼è¶…æ™‚\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è½‰ç¢¼éŒ¯èª¤: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_ytdlp_platform(platform_name, url, enable_mobile_conversion=False):\n",
    "    \"\"\"æ¸¬è©¦ yt-dlp é YouTube å¹³å°\"\"\"\n",
    "    print(f\"\\nğŸ§ª æ¸¬è©¦: yt-dlp - {platform_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"URL: {url}\\n\")\n",
    "        \n",
    "        # å„ªå…ˆé¸æ“‡æœ‰è¦–è¨Šçš„ MP4 æ ¼å¼\n",
    "        ydl_opts = {\n",
    "            'format': 'best[ext=mp4][vcodec!^=none][acodec!^=none]/best[vcodec!^=none][acodec!^=none]/best',\n",
    "            'outtmpl': str(OUTPUT_DIR / '%(title)s.%(ext)s'),  # æ·»åŠ å‰¯æª”å\n",
    "            'http_headers': {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            },\n",
    "            'quiet': False,\n",
    "            'no_warnings': False,\n",
    "        }\n",
    "        \n",
    "        if 'instagram' in platform_name.lower():\n",
    "            ydl_opts['extractor_args'] = {\n",
    "                'instagram': {'fetch_all_comments': False}\n",
    "            }\n",
    "        \n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            print(\"é–‹å§‹ä¸‹è¼‰...\")\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            \n",
    "            # ç²å–ä¸‹è¼‰çš„æ–‡ä»¶\n",
    "            ext = info.get('ext', 'mp4')\n",
    "            filename = f\"{info.get('title', 'video')}.{ext}\"\n",
    "            filepath = OUTPUT_DIR / filename\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            print(\"\\nâœ… ä¸‹è¼‰æˆåŠŸ:\")\n",
    "            print(f\"   æ¨™é¡Œ: {info.get('title', 'N/A')}\")\n",
    "            print(f\"   æ™‚é–“: {elapsed_time:.1f}s\")\n",
    "            print(f\"   æ–‡ä»¶: {filename}\")\n",
    "            print(f\"   å‰¯æª”å: .{ext}\")\n",
    "            \n",
    "            # MP4 è½‰ç¢¼ä»¥é©é…æ‰‹æ©Ÿåˆ†äº«\n",
    "            mobile_file = None\n",
    "            if enable_mobile_conversion and ext == 'mp4':\n",
    "                mobile_file = convert_mp4_for_mobile(str(filepath))\n",
    "            \n",
    "            test_results[f'ytdlp_{platform_name.replace(\" \", \"_\").lower()}'] = {\n",
    "                'status': 'âœ… æˆåŠŸ',\n",
    "                'title': info.get('title', 'N/A'),\n",
    "                'time': f\"{elapsed_time:.1f}s\",\n",
    "                'file': filename,\n",
    "                'mobile_file': Path(mobile_file).name if mobile_file else None\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {error_msg}\")\n",
    "        \n",
    "        test_results[f'ytdlp_{platform_name.replace(\" \", \"_\").lower()}'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {error_msg}'\n",
    "        }\n",
    "        \n",
    "        return False\n",
    "\n",
    "# yt-dlp åªç”¨æ–¼é YouTube å¹³å°\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª æ–¹æ¡ˆ 2: yt-dlp (Facebookã€Instagramã€X/Twitter)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤šå¹³å°æ¸¬è©¦\n",
    "\n",
    "æŒ‰é †åºæ¸¬è©¦å„å€‹å¹³å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆç”¨ pytubefix ä¸‹è¼‰ YouTube MP4\n",
    "print(\"\\nã€æ–¹æ¡ˆ 1: pytubefix - YouTubeã€‘\")\n",
    "test_pytubefix_youtube_mp4()\n",
    "\n",
    "# ç„¶å¾Œç”¨ yt-dlp ä¸‹è¼‰å…¶ä»–å¹³å°ï¼ˆå•Ÿç”¨æ‰‹æ©Ÿè½‰ç¢¼ï¼‰\n",
    "print(\"\\nã€æ–¹æ¡ˆ 2: yt-dlp - å…¶ä»–å¹³å°ã€‘\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "test_ytdlp_platform('Facebook', TEST_URLS['facebook'], enable_mobile_conversion=True)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "test_ytdlp_platform('Instagram', TEST_URLS['instagram'], enable_mobile_conversion=True)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "test_ytdlp_platform('X (Twitter)', TEST_URLS['twitter'], enable_mobile_conversion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threads ä¸‹è¼‰æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦ Threads å½±ç‰‡ä¸‹è¼‰çš„å¯è¡Œæ€§ã€‚ç”±æ–¼ yt-dlp å®˜æ–¹å°šæœªåˆä½µ Threads æ”¯æ´ï¼Œæˆ‘å€‘éœ€è¦ä½¿ç”¨é–‹ç™¼ä¸­çš„ extractor æˆ–æ‰‹å‹•æ–¹å¼ã€‚\n",
    "\n",
    "### æ–¹æ³•ä¸€ï¼šä½¿ç”¨é–‹ç™¼ç‰ˆ yt-dlp (å¾ PR branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def install_ytdlp_threads_branch():\n",
    "    \"\"\"å®‰è£åŒ…å« Threads extractor çš„é–‹ç™¼ç‰ˆ yt-dlp\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ”§ å®‰è£é–‹ç™¼ç‰ˆ yt-dlp (å« Threads æ”¯æ´)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # æ–¹æ³•1: å¾ Enucatl çš„ PR branch å®‰è£ (åŠŸèƒ½æœ€å®Œæ•´)\n",
    "    branch_url = \"https://github.com/Enucatl/yt-dlp/archive/refs/heads/threads.zip\"\n",
    "    \n",
    "    try:\n",
    "        # å…ˆå¸è¼‰ç¾æœ‰ç‰ˆæœ¬ï¼Œå†å®‰è£é–‹ç™¼ç‰ˆ\n",
    "        print(\"å¾ PR branch å®‰è£é–‹ç™¼ç‰ˆ yt-dlp...\")\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '--force-reinstall', f'git+https://github.com/Enucatl/yt-dlp.git@threads'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… é–‹ç™¼ç‰ˆ yt-dlp å®‰è£æˆåŠŸï¼\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ å®‰è£å¤±æ•—: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®‰è£ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_threads_with_ytdlp(url, use_cookies=False, cookies_file=None):\n",
    "    \"\"\"ä½¿ç”¨ yt-dlp æ¸¬è©¦ Threads ä¸‹è¼‰\"\"\"\n",
    "    print(f\"\\nğŸ§ª æ¸¬è©¦: yt-dlp Threads ä¸‹è¼‰\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"URL: {url}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from yt_dlp import YoutubeDL\n",
    "        \n",
    "        ydl_opts = {\n",
    "            'format': 'best[ext=mp4][vcodec!^=none][acodec!^=none]/best[vcodec!^=none][acodec!^=none]/best',\n",
    "            'outtmpl': str(OUTPUT_DIR / '%(title)s.%(ext)s'),\n",
    "            'http_headers': {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            },\n",
    "            'quiet': False,\n",
    "            'no_warnings': False,\n",
    "        }\n",
    "        \n",
    "        # å¦‚æœéœ€è¦ç™»å…¥ï¼Œä½¿ç”¨ cookies\n",
    "        if use_cookies and cookies_file:\n",
    "            ydl_opts['cookiefile'] = cookies_file\n",
    "            print(f\"ä½¿ç”¨ cookies æª”æ¡ˆ: {cookies_file}\")\n",
    "        \n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            # é¦–å…ˆåªæå–è³‡è¨Šï¼Œä¸ä¸‹è¼‰\n",
    "            print(\"\\nğŸ“‹ æå–å½±ç‰‡è³‡è¨Š...\")\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            \n",
    "            if info:\n",
    "                print(f\"\\nâœ… æˆåŠŸæå–è³‡è¨Š:\")\n",
    "                print(f\"   æ¨™é¡Œ: {info.get('title', 'N/A')}\")\n",
    "                print(f\"   ä½œè€…: {info.get('uploader', info.get('channel', 'N/A'))}\")\n",
    "                print(f\"   æè¿°: {(info.get('description', 'N/A') or 'N/A')[:100]}...\")\n",
    "                print(f\"   é¡å‹: {'è¼ªæ’­è²¼æ–‡' if info.get('_type') == 'playlist' else 'å–®ä¸€åª’é«”'}\")\n",
    "                \n",
    "                if info.get('_type') == 'playlist':\n",
    "                    print(f\"   é …ç›®æ•¸: {info.get('playlist_count', len(info.get('entries', [])))}\")\n",
    "                \n",
    "                # è©¢å•æ˜¯å¦ä¸‹è¼‰\n",
    "                print(\"\\nğŸ“¥ é–‹å§‹ä¸‹è¼‰...\")\n",
    "                info = ydl.extract_info(url, download=True)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                test_results['threads_ytdlp'] = {\n",
    "                    'status': 'âœ… æˆåŠŸ',\n",
    "                    'title': info.get('title', 'N/A'),\n",
    "                    'time': f\"{elapsed_time:.1f}s\",\n",
    "                }\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(\"ç„¡æ³•æå–å½±ç‰‡è³‡è¨Š\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"\\nâŒ ä¸‹è¼‰å¤±æ•—: {error_msg}\")\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æ˜¯èªè­‰å•é¡Œ\n",
    "        if 'login' in error_msg.lower() or 'private' in error_msg.lower():\n",
    "            print(\"\\nğŸ’¡ æç¤º: é€™å¯èƒ½æ˜¯ç§äººè²¼æ–‡ï¼Œéœ€è¦ç™»å…¥èªè­‰\")\n",
    "            print(\"   ä½¿ç”¨æ–¹å¼: --cookies cookies.txt æˆ– --cookies-from-browser chrome\")\n",
    "        \n",
    "        test_results['threads_ytdlp'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {error_msg}',\n",
    "        }\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# åŸ·è¡Œå®‰è£\n",
    "install_ytdlp_threads_branch()  # å–æ¶ˆè¨»è§£ä»¥å®‰è£é–‹ç™¼ç‰ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹æ³•äºŒï¼šæ‰‹å‹•è§£æ Threads é é¢è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_threads_manually(url, debug=False):\n",
    "    \"\"\"\n",
    "    æ‰‹å‹•è§£æ Threads é é¢è³‡æ–™ (ä¸ä¾è³´ yt-dlp Threads extractor)\n",
    "    \n",
    "    åŸç†:\n",
    "    1. ä¸‹è¼‰ Threads é é¢ HTML\n",
    "    2. å¾ <script type=\"application/json\" data-sjs> æ¨™ç±¤ä¸­æå– JSON è³‡æ–™\n",
    "    3. è§£æ RelayPrefetchedStreamCache ä¸­çš„è²¼æ–‡è³‡æ–™\n",
    "    4. æå–å½±ç‰‡ URL\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    import requests\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    print(f\"\\nğŸ§ª æ¸¬è©¦: æ‰‹å‹•è§£æ Threads\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"URL: {url}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # å¾ URL æå–è²¼æ–‡ ID\n",
    "        match = re.search(r'/(?:post|t)/([^/?#&]+)', url)\n",
    "        if not match:\n",
    "            raise Exception(\"ç„¡æ³•å¾ URL æå–è²¼æ–‡ ID\")\n",
    "        \n",
    "        post_id = match.group(1)\n",
    "        print(f\"è²¼æ–‡ ID: {post_id}\")\n",
    "        \n",
    "        # ä¸‹è¼‰é é¢\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "        }\n",
    "        \n",
    "        print(\"ä¸‹è¼‰é é¢...\")\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        webpage = response.text\n",
    "        print(f\"é é¢å¤§å°: {len(webpage)} bytes\")\n",
    "        \n",
    "        # æœå°‹åŒ…å«è²¼æ–‡è³‡æ–™çš„ JSON script\n",
    "        json_scripts = re.findall(\n",
    "            r'<script type=\"application/json\"[^>]*?\\sdata-sjs[^>]*?>(.*?)<\\s*/script\\s*>',\n",
    "            webpage,\n",
    "            re.DOTALL | re.IGNORECASE,\n",
    "        )\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(json_scripts)} å€‹ JSON script æ¨™ç±¤\")\n",
    "        \n",
    "        # å˜—è©¦å¤šç¨®è§£æç­–ç•¥\n",
    "        json_data = None\n",
    "        main_post = None\n",
    "        \n",
    "        for idx, script in enumerate(json_scripts):\n",
    "            # ç­–ç•¥ 1: æª¢æŸ¥æ˜¯å¦åŒ…å«è²¼æ–‡ ID å’Œé—œéµå­—\n",
    "            if post_id not in script:\n",
    "                continue\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\n  [Script {idx}] åŒ…å«è²¼æ–‡ IDï¼Œé•·åº¦: {len(script)} bytes\")\n",
    "            \n",
    "            try:\n",
    "                parsed = json.loads(script)\n",
    "                \n",
    "                # ç­–ç•¥ 2: å°‹æ‰¾åŒ…å« video_versions çš„è³‡æ–™\n",
    "                def find_video_data(data, depth=0):\n",
    "                    if depth > 30:\n",
    "                        return None\n",
    "                    \n",
    "                    if isinstance(data, dict):\n",
    "                        # æª¢æŸ¥æ˜¯å¦æœ‰ video_versions (å½±ç‰‡è³‡æ–™)\n",
    "                        if 'video_versions' in data and data.get('code') == post_id:\n",
    "                            return data\n",
    "                        if 'video_versions' in data and data.get('video_versions'):\n",
    "                            # å³ä½¿æ²’æœ‰ codeï¼Œä¹Ÿå¯èƒ½æ˜¯æˆ‘å€‘è¦çš„è³‡æ–™\n",
    "                            if debug:\n",
    "                                print(f\"    æ‰¾åˆ° video_versions åœ¨æ·±åº¦ {depth}\")\n",
    "                            return data\n",
    "                        \n",
    "                        for key, value in data.items():\n",
    "                            result = find_video_data(value, depth + 1)\n",
    "                            if result:\n",
    "                                return result\n",
    "                    \n",
    "                    elif isinstance(data, list):\n",
    "                        for item in data:\n",
    "                            result = find_video_data(item, depth + 1)\n",
    "                            if result:\n",
    "                                return result\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                result = find_video_data(parsed)\n",
    "                if result:\n",
    "                    main_post = result\n",
    "                    json_data = parsed\n",
    "                    print(f\"  âœ… åœ¨ Script {idx} ä¸­æ‰¾åˆ°å½±ç‰‡è³‡æ–™\")\n",
    "                    break\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                if debug:\n",
    "                    print(f\"  [Script {idx}] JSON è§£æå¤±æ•—: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # ç­–ç•¥ 3: å¦‚æœä¸Šé¢éƒ½æ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç›´æ¥å¾ HTML ä¸­æå–å½±ç‰‡ URL\n",
    "        if not main_post:\n",
    "            print(\"\\nå˜—è©¦ç›´æ¥å¾ HTML æå–å½±ç‰‡ URL...\")\n",
    "            \n",
    "            # å°‹æ‰¾ video_url æˆ– video_versions\n",
    "            video_url_match = re.search(\n",
    "                r'\"video_url\"\\s*:\\s*\"([^\"]+)\"',\n",
    "                webpage\n",
    "            )\n",
    "            \n",
    "            if video_url_match:\n",
    "                video_url = video_url_match.group(1).replace('\\\\u0026', '&').replace('\\\\/', '/')\n",
    "                print(f\"  æ‰¾åˆ° video_url: {video_url[:80]}...\")\n",
    "                \n",
    "                # ä¸‹è¼‰å½±ç‰‡\n",
    "                print(f\"\\nğŸ“¥ ä¸‹è¼‰å½±ç‰‡...\")\n",
    "                video_response = requests.get(video_url, headers=headers, stream=True)\n",
    "                \n",
    "                safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "                output_file = OUTPUT_DIR / f\"threads_{safe_title}.mp4\"\n",
    "                \n",
    "                with open(output_file, 'wb') as f:\n",
    "                    for chunk in video_response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"âœ… ä¸‹è¼‰å®Œæˆ: {output_file}\")\n",
    "                print(f\"   æª”æ¡ˆå¤§å°: {file_size:.2f} MB\")\n",
    "                print(f\"   èŠ±è²»æ™‚é–“: {elapsed_time:.1f}s\")\n",
    "                \n",
    "                test_results['threads_manual'] = {\n",
    "                    'status': 'âœ… æˆåŠŸ',\n",
    "                    'time': f\"{elapsed_time:.1f}s\",\n",
    "                    'size': f\"{file_size:.2f}MB\",\n",
    "                    'file': str(output_file),\n",
    "                }\n",
    "                return True\n",
    "            \n",
    "            # å˜—è©¦å°‹æ‰¾ CDN å½±ç‰‡é€£çµ\n",
    "            cdn_match = re.search(\n",
    "                r'(https?://[^\"]*?scontent[^\"]*?\\.mp4[^\"]*)',\n",
    "                webpage\n",
    "            )\n",
    "            \n",
    "            if cdn_match:\n",
    "                video_url = cdn_match.group(1).replace('\\\\u0026', '&').replace('\\\\/', '/')\n",
    "                print(f\"  æ‰¾åˆ° CDN video URL: {video_url[:80]}...\")\n",
    "                \n",
    "                # ä¸‹è¼‰å½±ç‰‡\n",
    "                print(f\"\\nğŸ“¥ ä¸‹è¼‰å½±ç‰‡...\")\n",
    "                video_response = requests.get(video_url, headers=headers, stream=True)\n",
    "                \n",
    "                safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "                output_file = OUTPUT_DIR / f\"threads_{safe_title}.mp4\"\n",
    "                \n",
    "                with open(output_file, 'wb') as f:\n",
    "                    for chunk in video_response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"âœ… ä¸‹è¼‰å®Œæˆ: {output_file}\")\n",
    "                print(f\"   æª”æ¡ˆå¤§å°: {file_size:.2f} MB\")\n",
    "                print(f\"   èŠ±è²»æ™‚é–“: {elapsed_time:.1f}s\")\n",
    "                \n",
    "                test_results['threads_manual'] = {\n",
    "                    'status': 'âœ… æˆåŠŸ',\n",
    "                    'time': f\"{elapsed_time:.1f}s\",\n",
    "                    'size': f\"{file_size:.2f}MB\",\n",
    "                    'file': str(output_file),\n",
    "                }\n",
    "                return True\n",
    "            \n",
    "            raise Exception(\"ç„¡æ³•å¾é é¢æå– JSON è³‡æ–™æˆ–å½±ç‰‡ URLï¼Œå¯èƒ½éœ€è¦ç™»å…¥\")\n",
    "        \n",
    "        print(\"âœ… æˆåŠŸæå– JSON è³‡æ–™\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ è²¼æ–‡è³‡è¨Š:\")\n",
    "        print(f\"   ä½œè€…: {main_post.get('user', {}).get('username', 'N/A')}\")\n",
    "        caption_text = main_post.get('caption', {})\n",
    "        if isinstance(caption_text, dict):\n",
    "            caption_text = caption_text.get('text', 'N/A')\n",
    "        print(f\"   èªªæ˜: {(caption_text or 'N/A')[:100]}...\")\n",
    "        \n",
    "        # æå–å½±ç‰‡ URL\n",
    "        video_urls = []\n",
    "        \n",
    "        def extract_video_urls(media_data):\n",
    "            if media_data.get('video_versions'):\n",
    "                for video in media_data.get('video_versions', []):\n",
    "                    if video.get('url'):\n",
    "                        video_urls.append({\n",
    "                            'url': video.get('url'),\n",
    "                            'width': video.get('width') or 0,\n",
    "                            'height': video.get('height') or 0,\n",
    "                        })\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦ç‚ºè¼ªæ’­\n",
    "        if main_post.get('carousel_media'):\n",
    "            print(f\"   é¡å‹: è¼ªæ’­è²¼æ–‡ ({len(main_post.get('carousel_media', []))} é …)\")\n",
    "            for media in main_post.get('carousel_media', []):\n",
    "                extract_video_urls(media)\n",
    "        else:\n",
    "            print(f\"   é¡å‹: å–®ä¸€åª’é«”\")\n",
    "            extract_video_urls(main_post)\n",
    "        \n",
    "        if video_urls:\n",
    "            print(f\"\\nğŸ“¹ æ‰¾åˆ° {len(video_urls)} å€‹å½±ç‰‡:\")\n",
    "            for i, video in enumerate(video_urls, 1):\n",
    "                w = video.get('width', 0) or 'N/A'\n",
    "                h = video.get('height', 0) or 'N/A'\n",
    "                print(f\"   {i}. {w}x{h}\")\n",
    "                print(f\"      URL: {video.get('url')[:80]}...\")\n",
    "            \n",
    "            # ä¸‹è¼‰ç¬¬ä¸€å€‹å½±ç‰‡ï¼ˆæˆ–æœ€é«˜å“è³ªï¼‰\n",
    "            # ä½¿ç”¨å®‰å…¨çš„æ¯”è¼ƒæ–¹å¼\n",
    "            def get_video_size(v):\n",
    "                w = v.get('width') or 0\n",
    "                h = v.get('height') or 0\n",
    "                return w * h\n",
    "            \n",
    "            best_video = max(video_urls, key=get_video_size) if video_urls else video_urls[0]\n",
    "            \n",
    "            print(f\"\\nğŸ“¥ ä¸‹è¼‰å½±ç‰‡...\")\n",
    "            video_response = requests.get(best_video['url'], headers=headers, stream=True)\n",
    "            \n",
    "            safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "            output_file = OUTPUT_DIR / f\"threads_{safe_title}.mp4\"\n",
    "            \n",
    "            with open(output_file, 'wb') as f:\n",
    "                for chunk in video_response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"âœ… ä¸‹è¼‰å®Œæˆ: {output_file}\")\n",
    "            print(f\"   æª”æ¡ˆå¤§å°: {file_size:.2f} MB\")\n",
    "            print(f\"   èŠ±è²»æ™‚é–“: {elapsed_time:.1f}s\")\n",
    "            \n",
    "            test_results['threads_manual'] = {\n",
    "                'status': 'âœ… æˆåŠŸ',\n",
    "                'time': f\"{elapsed_time:.1f}s\",\n",
    "                'size': f\"{file_size:.2f}MB\",\n",
    "                'file': str(output_file),\n",
    "            }\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ æ­¤è²¼æ–‡ä¸åŒ…å«å½±ç‰‡\")\n",
    "            test_results['threads_manual'] = {\n",
    "                'status': 'âš ï¸ ç„¡å½±ç‰‡å…§å®¹',\n",
    "            }\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"\\nâŒ è§£æå¤±æ•—: {error_msg}\")\n",
    "        \n",
    "        test_results['threads_manual'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {error_msg}',\n",
    "        }\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "parse_threads_manually(TEST_URLS[\"threads_public\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹æ³•ä¸‰ï¼š ä½¿ç”¨ Instagram Cookies èªè­‰ Threads\n",
    "\n",
    "Threads èˆ‡ Instagram åŒå±¬ Meta å¹³å°ï¼Œå…±ç”¨èªè­‰ç³»çµ±ã€‚å¯ä½¿ç”¨ Instagram cookies ä¾†å­˜å–éœ€è¦ç™»å…¥çš„ Threads å…§å®¹ã€‚\n",
    "\n",
    "#### æ­¥é©Ÿï¼š\n",
    "1. åœ¨ç€è¦½å™¨ç™»å…¥ Instagram/Threads\n",
    "2. ä½¿ç”¨ç€è¦½å™¨æ“´å……å¥—ä»¶åŒ¯å‡º cookies (å¦‚ \"Get cookies.txt LOCALLY\")\n",
    "3. æˆ–ä½¿ç”¨ yt-dlp çš„ `--cookies-from-browser chrome` åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_threads_with_cookies(url, cookies_path=None, debug=False):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ cookies è§£æ Threads é é¢è³‡æ–™ï¼ˆæ”¯æ´ç§äººè²¼æ–‡ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        url: Threads è²¼æ–‡ URL\n",
    "        cookies_path: cookies æª”æ¡ˆè·¯å¾‘ (Netscape æ ¼å¼)\n",
    "        debug: æ˜¯å¦è¼¸å‡ºé™¤éŒ¯è³‡è¨Š\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    import requests\n",
    "    from http.cookiejar import MozillaCookieJar\n",
    "    \n",
    "    print(f\"\\nğŸ§ª æ¸¬è©¦: æ‰‹å‹•è§£æ Threads (å« cookies)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"URL: {url}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # å¾ URL æå–è²¼æ–‡ ID\n",
    "        match = re.search(r'/(?:post|t)/([^/?#&]+)', url)\n",
    "        if not match:\n",
    "            raise Exception(\"ç„¡æ³•å¾ URL æå–è²¼æ–‡ ID\")\n",
    "        \n",
    "        post_id = match.group(1)\n",
    "        print(f\"è²¼æ–‡ ID: {post_id}\")\n",
    "        \n",
    "        # è¨­å®š session\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # è¼‰å…¥ cookies\n",
    "        if cookies_path and Path(cookies_path).exists():\n",
    "            print(f\"è¼‰å…¥ cookies: {cookies_path}\")\n",
    "            try:\n",
    "                cookie_jar = MozillaCookieJar(str(cookies_path))\n",
    "                cookie_jar.load(ignore_discard=True, ignore_expires=True)\n",
    "                session.cookies = cookie_jar\n",
    "                print(f\"   å·²è¼‰å…¥ {len(cookie_jar)} å€‹ cookies\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ è¼‰å…¥ cookies å¤±æ•—: {e}\")\n",
    "        \n",
    "        # ä¸‹è¼‰é é¢\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "        }\n",
    "        \n",
    "        print(\"ä¸‹è¼‰é é¢...\")\n",
    "        response = session.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        webpage = response.text\n",
    "        print(f\"é é¢å¤§å°: {len(webpage)} bytes\")\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦éœ€è¦ç™»å…¥\n",
    "        if 'login' in webpage.lower() and 'Log in to see this post' in webpage:\n",
    "            print(\"âš ï¸ æ­¤è²¼æ–‡éœ€è¦ç™»å…¥æ‰èƒ½æŸ¥çœ‹\")\n",
    "            if not cookies_path:\n",
    "                raise Exception(\"éœ€è¦ç™»å…¥èªè­‰ï¼Œè«‹æä¾› cookies æª”æ¡ˆ\")\n",
    "        \n",
    "        # æœå°‹åŒ…å«è²¼æ–‡è³‡æ–™çš„ JSON script\n",
    "        json_scripts = re.findall(\n",
    "            r'<script type=\"application/json\"[^>]*?\\sdata-sjs[^>]*?>(.*?)<\\s*/script\\s*>',\n",
    "            webpage,\n",
    "            re.DOTALL | re.IGNORECASE,\n",
    "        )\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(json_scripts)} å€‹ JSON script æ¨™ç±¤\")\n",
    "        \n",
    "        # è§£æç­–ç•¥\n",
    "        main_post = None\n",
    "        video_urls = []\n",
    "        image_urls = []\n",
    "        \n",
    "        for idx, script in enumerate(json_scripts):\n",
    "            if post_id not in script:\n",
    "                continue\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\n  [Script {idx}] åŒ…å«è²¼æ–‡ IDï¼Œé•·åº¦: {len(script)} bytes\")\n",
    "            \n",
    "            try:\n",
    "                parsed = json.loads(script)\n",
    "                \n",
    "                def find_media_data(data, depth=0):\n",
    "                    if depth > 30:\n",
    "                        return None\n",
    "                    \n",
    "                    if isinstance(data, dict):\n",
    "                        # æª¢æŸ¥å½±ç‰‡\n",
    "                        if 'video_versions' in data and data.get('video_versions'):\n",
    "                            return data\n",
    "                        # æª¢æŸ¥åœ–ç‰‡\n",
    "                        if 'image_versions2' in data and not data.get('video_versions'):\n",
    "                            return data\n",
    "                        \n",
    "                        for key, value in data.items():\n",
    "                            result = find_media_data(value, depth + 1)\n",
    "                            if result:\n",
    "                                return result\n",
    "                    \n",
    "                    elif isinstance(data, list):\n",
    "                        for item in data:\n",
    "                            result = find_media_data(item, depth + 1)\n",
    "                            if result:\n",
    "                                return result\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                # å°‹æ‰¾æ‰€æœ‰åª’é«”\n",
    "                def find_all_media(data, media_list=None, depth=0):\n",
    "                    if media_list is None:\n",
    "                        media_list = []\n",
    "                    if depth > 30:\n",
    "                        return media_list\n",
    "                    \n",
    "                    if isinstance(data, dict):\n",
    "                        if 'video_versions' in data and data.get('video_versions'):\n",
    "                            media_list.append({'type': 'video', 'data': data})\n",
    "                        elif 'image_versions2' in data and data.get('image_versions2'):\n",
    "                            media_list.append({'type': 'image', 'data': data})\n",
    "                        \n",
    "                        for key, value in data.items():\n",
    "                            find_all_media(value, media_list, depth + 1)\n",
    "                    \n",
    "                    elif isinstance(data, list):\n",
    "                        for item in data:\n",
    "                            find_all_media(item, media_list, depth + 1)\n",
    "                    \n",
    "                    return media_list\n",
    "                \n",
    "                media_items = find_all_media(parsed)\n",
    "                if media_items:\n",
    "                    print(f\"  âœ… åœ¨ Script {idx} ä¸­æ‰¾åˆ° {len(media_items)} å€‹åª’é«”\")\n",
    "                    \n",
    "                    for media in media_items:\n",
    "                        if media['type'] == 'video':\n",
    "                            for video in media['data'].get('video_versions', []):\n",
    "                                if video.get('url'):\n",
    "                                    video_urls.append({\n",
    "                                        'url': video.get('url'),\n",
    "                                        'width': video.get('width') or 0,\n",
    "                                        'height': video.get('height') or 0,\n",
    "                                    })\n",
    "                        elif media['type'] == 'image':\n",
    "                            candidates = media['data'].get('image_versions2', {}).get('candidates', [])\n",
    "                            for img in candidates:\n",
    "                                if img.get('url'):\n",
    "                                    image_urls.append({\n",
    "                                        'url': img.get('url'),\n",
    "                                        'width': img.get('width') or 0,\n",
    "                                        'height': img.get('height') or 0,\n",
    "                                    })\n",
    "                    \n",
    "                    if video_urls or image_urls:\n",
    "                        break\n",
    "                        \n",
    "            except json.JSONDecodeError as e:\n",
    "                if debug:\n",
    "                    print(f\"  [Script {idx}] JSON è§£æå¤±æ•—: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # ä¸‹è¼‰åª’é«”\n",
    "        downloaded_files = []\n",
    "        \n",
    "        if video_urls:\n",
    "            print(f\"\\nğŸ“¹ æ‰¾åˆ° {len(video_urls)} å€‹å½±ç‰‡ç‰ˆæœ¬\")\n",
    "            \n",
    "            # é¸æ“‡æœ€é«˜å“è³ª\n",
    "            def get_video_size(v):\n",
    "                w = v.get('width') or 0\n",
    "                h = v.get('height') or 0\n",
    "                return w * h\n",
    "            \n",
    "            best_video = max(video_urls, key=get_video_size)\n",
    "            \n",
    "            print(f\"ğŸ“¥ ä¸‹è¼‰å½±ç‰‡...\")\n",
    "            video_response = session.get(best_video['url'], headers=headers, stream=True)\n",
    "            \n",
    "            safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "            output_file = OUTPUT_DIR / f\"threads_{safe_title}.mp4\"\n",
    "            \n",
    "            with open(output_file, 'wb') as f:\n",
    "                for chunk in video_response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ… å½±ç‰‡ä¸‹è¼‰å®Œæˆ: {output_file.name} ({file_size:.2f} MB)\")\n",
    "            downloaded_files.append(str(output_file))\n",
    "        \n",
    "        elif image_urls:\n",
    "            print(f\"\\nğŸ–¼ï¸ æ‰¾åˆ° {len(image_urls)} å€‹åœ–ç‰‡ç‰ˆæœ¬\")\n",
    "            \n",
    "            # é¸æ“‡æœ€é«˜å“è³ª\n",
    "            def get_image_size(v):\n",
    "                w = v.get('width') or 0\n",
    "                h = v.get('height') or 0\n",
    "                return w * h\n",
    "            \n",
    "            best_image = max(image_urls, key=get_image_size)\n",
    "            \n",
    "            print(f\"ğŸ“¥ ä¸‹è¼‰åœ–ç‰‡...\")\n",
    "            image_response = session.get(best_image['url'], headers=headers, stream=True)\n",
    "            \n",
    "            safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "            output_file = OUTPUT_DIR / f\"threads_{safe_title}.jpg\"\n",
    "            \n",
    "            with open(output_file, 'wb') as f:\n",
    "                for chunk in image_response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ… åœ–ç‰‡ä¸‹è¼‰å®Œæˆ: {output_file.name} ({file_size:.2f} MB)\")\n",
    "            downloaded_files.append(str(output_file))\n",
    "        \n",
    "        else:\n",
    "            # å˜—è©¦ç›´æ¥å¾ HTML æå–\n",
    "            print(\"\\nå˜—è©¦ç›´æ¥å¾ HTML æå–åª’é«” URL...\")\n",
    "            \n",
    "            video_match = re.search(r'\"video_url\"\\s*:\\s*\"([^\"]+)\"', webpage)\n",
    "            if video_match:\n",
    "                video_url = video_match.group(1).replace('\\\\u0026', '&').replace('\\\\/', '/')\n",
    "                print(f\"  æ‰¾åˆ° video_url\")\n",
    "                \n",
    "                video_response = session.get(video_url, headers=headers, stream=True)\n",
    "                safe_title = re.sub(r'[<>:\"/\\\\|?*]', '_', post_id)\n",
    "                output_file = OUTPUT_DIR / f\"threads_{safe_title}.mp4\"\n",
    "                \n",
    "                with open(output_file, 'wb') as f:\n",
    "                    for chunk in video_response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                file_size = output_file.stat().st_size / (1024 * 1024)\n",
    "                print(f\"âœ… å½±ç‰‡ä¸‹è¼‰å®Œæˆ: {output_file.name} ({file_size:.2f} MB)\")\n",
    "                downloaded_files.append(str(output_file))\n",
    "            else:\n",
    "                raise Exception(\"ç„¡æ³•æ‰¾åˆ°å½±ç‰‡æˆ–åœ–ç‰‡è³‡æ–™ï¼Œå¯èƒ½éœ€è¦ç™»å…¥æˆ–æ­¤è²¼æ–‡ä¸åŒ…å«åª’é«”\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nâœ… å®Œæˆ! å…±ä¸‹è¼‰ {len(downloaded_files)} å€‹æª”æ¡ˆï¼Œè€—æ™‚ {elapsed_time:.1f}s\")\n",
    "        \n",
    "        test_results['threads_cookies'] = {\n",
    "            'status': 'âœ… æˆåŠŸ',\n",
    "            'time': f\"{elapsed_time:.1f}s\",\n",
    "            'files': downloaded_files,\n",
    "        }\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"\\nâŒ è§£æå¤±æ•—: {error_msg}\")\n",
    "        \n",
    "        test_results['threads_cookies'] = {\n",
    "            'status': f'âŒ å¤±æ•—: {error_msg}',\n",
    "        }\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_threads_with_cookies():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Instagram/Threads cookies æ¸¬è©¦ç§äººè²¼æ–‡ä¸‹è¼‰\n",
    "    \"\"\"\n",
    "    cookies_path = Path('../cookies/threads.txt')\n",
    "    \n",
    "    # ä¹Ÿå¯ä»¥ä½¿ç”¨ Instagram cookies\n",
    "    if not cookies_path.exists():\n",
    "        cookies_path = Path('../cookies/instagram.txt')\n",
    "    \n",
    "    if not cookies_path.exists():\n",
    "        print(\"âš ï¸ æ‰¾ä¸åˆ° cookies æª”æ¡ˆ\")\n",
    "        print(\"   è«‹åœ¨ä»¥ä¸‹ä½ç½®å»ºç«‹ cookies æª”æ¡ˆ:\")\n",
    "        print(f\"   - {Path('../cookies/threads.txt').absolute()}\")\n",
    "        print(f\"   - {Path('../cookies/instagram.txt').absolute()}\")\n",
    "        print(\"\\n   å»ºç«‹æ­¥é©Ÿ:\")\n",
    "        print(\"   1. å®‰è£ç€è¦½å™¨æ“´å……å¥—ä»¶ 'Get cookies.txt LOCALLY'\")\n",
    "        print(\"   2. ç™»å…¥ threads.com æˆ– instagram.com\")\n",
    "        print(\"   3. é»æ“Šæ“´å……å¥—ä»¶åŒ¯å‡º cookies\")\n",
    "        print(\"   4. å°‡æª”æ¡ˆå­˜æ”¾æ–¼ä¸Šè¿°è·¯å¾‘\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"âœ… æ‰¾åˆ° cookies æª”æ¡ˆ: {cookies_path}\")\n",
    "    \n",
    "    # ä½¿ç”¨æ‰‹å‹•è§£ææ–¹å¼ (å› ç‚º yt-dlp å®˜æ–¹å°šæœªæ”¯æ´ Threads)\n",
    "    test_url = TEST_URLS.get('threads_private', TEST_URLS['threads_public'])\n",
    "    \n",
    "    return parse_threads_with_cookies(test_url, cookies_path=str(cookies_path.absolute()))\n",
    "\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_threads_with_cookies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
